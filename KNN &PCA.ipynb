{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM0dLa81+EcBZfqE3PSpwUe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#KNN & PCA | Assignment\n","\n","#Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n","\n","**Definition:**  \n","K-Nearest Neighbors (KNN) is a **non-parametric, instance-based learning algorithm** used for both classification and regression.  \n","It predicts the output of a new data point based on the **majority label (classification)** or **average value (regression)** of its 'K' nearest neighbors in the training dataset.\n","\n","---\n","\n","**How it works:**\n","\n","### 1. Classification:\n","1. Choose a value of `K` (number of neighbors to consider).  \n","2. Calculate the **distance** between the new data point and all training points (commonly Euclidean distance).  \n","3. Identify the `K` nearest neighbors.  \n","4. Assign the **most frequent class** among the neighbors to the new data point.  \n","\n","**Example:**  \n","- New point → Find 5 nearest neighbors → Class counts: {Class A: 3, Class B: 2} → Predict **Class A**.\n","\n","### 2. Regression:\n","1. Same as classification: find `K` nearest neighbors using distance.  \n","2. Take the **average (or weighted average) of the neighbors’ target values**.  \n","3. Assign this as the predicted value for the new point.\n","\n","**Example:**  \n","- New point → Find 3 nearest neighbors → Target values: [5.2, 4.8, 6.0] → Predict **(5.2 + 4.8 + 6.0)/3 = 5.33**.\n","\n","---\n","\n","**Key Points:**\n","- **Non-parametric:** No assumption about data distribution.  \n","- **Lazy learner:** No training phase; all computation happens during prediction.  \n","- **Distance metric matters:** Common choices – Euclidean, Manhattan, Minkowski.  \n","- **Choice of K:** Small K → sensitive to noise; Large K → smoother predictions but may miss local patterns.\n","\n","---\n","#Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n","\n","**Definition:**  \n","The **Curse of Dimensionality** refers to the set of problems that arise when working with **high-dimensional data** (many features).  \n","As the number of dimensions increases, the **volume of the feature space grows exponentially**, making the data points **sparser**.  \n","\n","---\n","\n","**Effect on K-Nearest Neighbors (KNN):**\n","\n","1. **Distance Metrics Become Less Meaningful:**\n","   - KNN relies on distance (e.g., Euclidean) to find neighbors.\n","   - In high dimensions, **all points tend to become almost equidistant**.\n","   - This makes it difficult to identify truly \"nearest\" neighbors.\n","\n","2. **Increased Computational Cost:**\n","   - More dimensions → more calculations for distance → slower prediction.\n","\n","3. **Overfitting Risk:**\n","   - Sparse high-dimensional data may cause KNN to **fit noise rather than patterns**.\n","   - Small K values become unstable; predictions become less reliable.\n","\n","4. **Reduced Predictive Power:**\n","   - High dimensionality can **dilute the effect of relevant features**.\n","   - KNN performance often drops unless dimensionality reduction is applied.\n","\n","---\n","\n","**Mitigation Strategies:**\n","- **Feature Selection:** Keep only the most relevant features.  \n","- **Dimensionality Reduction:** Use PCA, t-SNE, or other techniques.  \n","- **Increase Training Data:** More samples can help reduce sparsity effects.  \n","- **Distance Weighting:** Weight closer neighbors more heavily to reduce noise impact.\n","\n","---\n","#Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n","\n","**Definition of PCA:**  \n","Principal Component Analysis (PCA) is an **unsupervised dimensionality reduction technique** that transforms the original correlated features into a **new set of uncorrelated features** called **principal components (PCs)**.  \n","- Each PC is a linear combination of the original features.  \n","- The first few PCs capture the **maximum variance** in the data.\n","\n","**How PCA works:**\n","1. Standardize the dataset (mean=0, variance=1).  \n","2. Compute the **covariance matrix** of the features.  \n","3. Calculate **eigenvectors and eigenvalues** of the covariance matrix.  \n","4. Sort eigenvectors by decreasing eigenvalues → top eigenvectors become principal components.  \n","5. Project the original data onto these principal components to reduce dimensionality.\n","\n","---\n","\n","**Difference from Feature Selection:**\n","\n","| Aspect               | PCA                                      | Feature Selection                     |\n","|----------------------|-----------------------------------------|--------------------------------------|\n","| Type                 | Feature **extraction**                  | Feature **selection**                 |\n","| Output               | New features (principal components)     | Subset of original features           |\n","| Goal                 | Reduce dimensionality while preserving **variance** | Keep most **informative/relevant features** |\n","| Method               | Linear combination of all features      | Evaluate features using metrics (e.g., correlation, mutual information, model importance) |\n","| Interpretability     | Less interpretable (PCs are combinations)| More interpretable (original features)|\n","\n","**Key Point:**  \n","- PCA transforms data into a new space (features are combinations).  \n","- Feature selection chooses a **subset of existing features** without changing them.\n","\n","---\n","#Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n","\n","\n","**Definition:**\n","\n","- **Eigenvectors:** Directions in the feature space along which the data varies the most.  \n","  - In PCA, each eigenvector represents a **principal component**.  \n","  - They define the new axes after dimensionality reduction.\n","\n","- **Eigenvalues:** Magnitudes corresponding to eigenvectors that indicate **how much variance is captured** along each principal component.  \n","  - Larger eigenvalues → more variance explained along that eigenvector.  \n","\n","---\n","\n","**Why are they important in PCA?**\n","\n","1. **Determine Principal Components:**\n","   - Eigenvectors define the new coordinate system (principal components) for the data.\n","   \n","2. **Measure Importance of Components:**\n","   - Eigenvalues show **how much information (variance)** each component carries.\n","   - Components with small eigenvalues can often be discarded to reduce dimensionality without losing much information.\n","\n","3. **Dimensionality Reduction:**\n","   - By selecting the top-k eigenvectors with largest eigenvalues, PCA projects data onto a lower-dimensional space while **retaining most of the variance**.\n","\n","---\n","\n","**Example Conceptually:**\n","- Original 3D data → PCA finds eigenvectors: `[v1, v2, v3]`  \n","- Eigenvalues: `[5.0, 2.0, 0.1]`  \n","- Project data onto top 2 eigenvectors (`v1` and `v2`) → reduces to 2D while keeping ~97% of variance.\n","\n","---\n","#Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n","\n","**Combining KNN and PCA:**\n","\n","1. **Challenge with KNN in High Dimensions:**\n","   - KNN relies on distance metrics (e.g., Euclidean distance).  \n","   - High-dimensional data suffers from the **Curse of Dimensionality** → distances become less meaningful, KNN performance drops.\n","\n","2. **Role of PCA:**\n","   - PCA reduces dimensionality by projecting data onto **principal components** that retain most of the variance.  \n","   - This removes noisy or less informative features and **reduces sparsity** in high-dimensional space.\n","\n","3. **Pipeline Benefits:**\n","   - **Step 1:** Apply PCA → compress features to lower-dimensional space.  \n","   - **Step 2:** Apply KNN → distances now computed on fewer, more informative dimensions.  \n","   - **Advantages:**  \n","     - Faster computation for KNN.  \n","     - More robust predictions due to reduced noise.  \n","     - Mitigates the Curse of Dimensionality.  \n","\n","---\n","\n","**Example Conceptually:**\n","- Original dataset: 50 features → PCA reduces to 10 principal components  \n","- KNN is applied on these 10 components instead of 50 features → better classification/regression performance.\n","\n","**Summary:**  \n","- PCA acts as a **preprocessing step** that improves KNN efficiency and accuracy, especially on high-dimensional datasets.\n","\n","---\n","#Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n"],"metadata":{"id":"vo4kaVEGLYes"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load dataset\n","data = load_wine()\n","X, y = data.data, data.target\n","\n","# 2. Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42, stratify=y\n",")\n","\n","# ------------------------------\n","# 3. KNN without feature scaling\n","# ------------------------------\n","knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n","knn_no_scaling.fit(X_train, y_train)\n","y_pred_no_scaling = knn_no_scaling.predict(X_test)\n","acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n","\n","# ------------------------------\n","# 4. KNN with feature scaling\n","# ------------------------------\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","knn_scaled = KNeighborsClassifier(n_neighbors=5)\n","knn_scaled.fit(X_train_scaled, y_train)\n","y_pred_scaled = knn_scaled.predict(X_test_scaled)\n","acc_scaled = accuracy_score(y_test, y_pred_scaled)\n","\n","# ------------------------------\n","# 5. Compare accuracies\n","# ------------------------------\n","print(\"KNN Accuracy without scaling:\", acc_no_scaling)\n","print(\"KNN Accuracy with scaling:\", acc_scaled)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00fQGpGpNYfG","executionInfo":{"status":"ok","timestamp":1757249149741,"user_tz":-330,"elapsed":2660,"user":{"displayName":"Shubham Yadav","userId":"17612701105650508696"}},"outputId":"7dbdac64-9dd3-4e8c-8b36-cfec47106562"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["KNN Accuracy without scaling: 0.7222222222222222\n","KNN Accuracy with scaling: 0.9444444444444444\n"]}]},{"cell_type":"markdown","source":["#Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n"],"metadata":{"id":"kthbpUsxNoNn"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","# 1. Load dataset\n","data = load_wine()\n","X = data.data\n","\n","# 2. Standardize features (important for PCA)\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# 3. Train PCA model\n","pca = PCA()\n","pca.fit(X_scaled)\n","\n","# 4. Print explained variance ratio\n","explained_variance_ratio = pca.explained_variance_ratio_\n","for i, ratio in enumerate(explained_variance_ratio, start=1):\n","    print(f\"PC{i}: {ratio:.4f}\")\n","\n","# Optional: cumulative variance\n","import numpy as np\n","cumulative_variance = np.cumsum(explained_variance_ratio)\n","print(\"\\nCumulative Variance Explained:\", cumulative_variance)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K_ycRlISN7b9","executionInfo":{"status":"ok","timestamp":1757249258364,"user_tz":-330,"elapsed":54,"user":{"displayName":"Shubham Yadav","userId":"17612701105650508696"}},"outputId":"a26197ea-7963-4a36-adf2-85bbaedbaf03"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["PC1: 0.3620\n","PC2: 0.1921\n","PC3: 0.1112\n","PC4: 0.0707\n","PC5: 0.0656\n","PC6: 0.0494\n","PC7: 0.0424\n","PC8: 0.0268\n","PC9: 0.0222\n","PC10: 0.0193\n","PC11: 0.0174\n","PC12: 0.0130\n","PC13: 0.0080\n","\n","Cumulative Variance Explained: [0.36198848 0.55406338 0.66529969 0.73598999 0.80162293 0.85098116\n"," 0.89336795 0.92017544 0.94239698 0.96169717 0.97906553 0.99204785\n"," 1.        ]\n"]}]},{"cell_type":"markdown","source":["#Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n"],"metadata":{"id":"O6YCp3frN-gs"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load dataset\n","data = load_wine()\n","X, y = data.data, data.target\n","\n","# 2. Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, stratify=y, random_state=42\n",")\n","\n","# 3. Standardize features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# 4. KNN on original dataset\n","knn_original = KNeighborsClassifier(n_neighbors=5)\n","knn_original.fit(X_train_scaled, y_train)\n","y_pred_original = knn_original.predict(X_test_scaled)\n","acc_original = accuracy_score(y_test, y_pred_original)\n","\n","# 5. PCA transformation (retain top 2 components)\n","pca = PCA(n_components=2)\n","X_train_pca = pca.fit_transform(X_train_scaled)\n","X_test_pca = pca.transform(X_test_scaled)\n","\n","# 6. KNN on PCA-transformed dataset\n","knn_pca = KNeighborsClassifier(n_neighbors=5)\n","knn_pca.fit(X_train_pca, y_train)\n","y_pred_pca = knn_pca.predict(X_test_pca)\n","acc_pca = accuracy_score(y_test, y_pred_pca)\n","\n","# 7. Compare accuracies\n","print(\"KNN Accuracy on Original Dataset:\", acc_original)\n","print(\"KNN Accuracy on PCA-Transformed Dataset (2 PCs):\", acc_pca)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-NM-yCPvOLZs","executionInfo":{"status":"ok","timestamp":1757249323921,"user_tz":-330,"elapsed":31,"user":{"displayName":"Shubham Yadav","userId":"17612701105650508696"}},"outputId":"11756a4b-ce59-431c-85f8-fbf86178237a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["KNN Accuracy on Original Dataset: 0.9444444444444444\n","KNN Accuracy on PCA-Transformed Dataset (2 PCs): 0.9444444444444444\n"]}]},{"cell_type":"markdown","source":["#Question 9: Train a KNN Classifier with different distance metrics (euclidean,manhattan) on the scaled Wine dataset and compare the results."],"metadata":{"id":"PW4C19lTOSCN"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load dataset\n","data = load_wine()\n","X, y = data.data, data.target\n","\n","# 2. Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, stratify=y, random_state=42\n",")\n","\n","# 3. Standardize features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# ------------------------------\n","# 4. KNN with Euclidean distance\n","# ------------------------------\n","knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n","knn_euclidean.fit(X_train_scaled, y_train)\n","y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n","acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n","\n","# ------------------------------\n","# 5. KNN with Manhattan distance\n","# ------------------------------\n","knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n","knn_manhattan.fit(X_train_scaled, y_train)\n","y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n","acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n","\n","# ------------------------------\n","# 6. Compare results\n","# ------------------------------\n","print(\"KNN Accuracy with Euclidean distance:\", acc_euclidean)\n","print(\"KNN Accuracy with Manhattan distance:\", acc_manhattan)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iUIxDmVyNfrq","executionInfo":{"status":"ok","timestamp":1757249414036,"user_tz":-330,"elapsed":43,"user":{"displayName":"Shubham Yadav","userId":"17612701105650508696"}},"outputId":"0b544ddb-aba0-4551-f353-d5bd124a173a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["KNN Accuracy with Euclidean distance: 0.9444444444444444\n","KNN Accuracy with Manhattan distance: 0.9814814814814815\n"]}]},{"cell_type":"markdown","source":["#Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.Due to the large number of features and a small number of samples, traditional models overfit.\n","#Explain how you would:\n","#● Use PCA to reduce dimensionality\n","#● Decide how many components to keep\n","#● Use KNN for classification post-dimensionality reduction\n","#● Evaluate the model\n","#● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n","\n","**Scenario:**  \n","- Dataset: High-dimensional gene expression data  \n","- Problem: Large number of features (thousands of genes) but small number of patient samples → risk of **overfitting**.  \n","- Goal: Classify patients into cancer types using KNN.\n","\n","---\n","\n","## Step-by-Step Approach:\n","\n","### 1. Use PCA to Reduce Dimensionality\n","- Apply **Principal Component Analysis (PCA)** to transform the original high-dimensional features into a **smaller set of uncorrelated principal components**.  \n","- Benefits:  \n","  - Reduces noise and redundancy in features.  \n","  - Helps KNN perform better by mitigating the Curse of Dimensionality.\n","\n","### 2. Decide How Many Components to Keep\n","- Use the **explained variance ratio** from PCA.  \n","- Keep the minimum number of components that **capture ~90-95% of the total variance**.  \n","- This balances dimensionality reduction with retaining essential information.\n","\n","### 3. Use KNN for Classification Post-Dimensionality Reduction\n","- Standardize the data before PCA.  \n","- Fit KNN on the **PCA-transformed training set**.  \n","- Use distance-based metrics (Euclidean or Manhattan) to classify patients based on nearest neighbors in the reduced feature space.\n","\n","### 4. Evaluate the Model\n","- Split dataset into **train and test sets** or use **cross-validation**.  \n","- Evaluate using metrics appropriate for multi-class classification:  \n","  - **Accuracy**  \n","  - **F1-score (macro or weighted)**  \n","  - **Confusion matrix** to see misclassification patterns.  \n","  - Optional: ROC-AUC for one-vs-rest classification.\n","\n","### 5. Justify this Pipeline to Stakeholders\n","- **Dimensionality Reduction:** PCA reduces thousands of genes to a manageable number of components → prevents overfitting.  \n","- **Robust Classification:** KNN is simple, interpretable, and works well on the transformed low-dimensional space.  \n","- **Reproducibility:** Pipeline is systematic and can be applied to new patient samples.  \n","- **Biomedical Relevance:** Captures major variance patterns in gene expression while filtering noise, improving generalization.  \n","\n","**Conclusion:**  \n","- Using PCA + KNN forms a **robust, interpretable pipeline** for high-dimensional biomedical data, minimizing overfitting while retaining predictive power."],"metadata":{"id":"a0kD4MoJOpAP"}}]}